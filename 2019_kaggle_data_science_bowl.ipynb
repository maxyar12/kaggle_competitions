{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle 2019 Data Science Bowl\n",
    "\n",
    "This notebook contains my submissions to the 2019 Kaggle Data Science Bowl, a supervised learning competition. The data comes from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors were challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes.\n",
    "\n",
    "The work is organized as follows:\n",
    "\n",
    "1. Data Load and Inspection\n",
    "2. EDA\n",
    "3. Ground Truth\n",
    "4. Feature engineering \n",
    "5. Feature transformations\n",
    "5. Pre-processing \n",
    "6. Evaluation: Quadratic Weighted Kappa (qwk)\n",
    "7. Models\n",
    "    - KNN Model\n",
    "    - SVM Model\n",
    "    - Decision Tree Model(single)\n",
    "    - Gradient Boosted Tree Model\n",
    "8. Discussion and Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline\n",
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '../../../../dsb_data/'\n",
    "train = pd.read_csv(basepath + 'processed/train.csv/train.csv') #df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(basepath + 'processed/test.csv/test.csv') #df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.csv file has 11341042 rows and 11 columns\n"
     ]
    }
   ],
   "source": [
    "print('Training.csv file has {} rows and {} columns'.format(train.shape[0], train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test.csv file has 1156414 rows and 11 columns\n"
     ]
    }
   ],
   "source": [
    "print('Test.csv file has {} rows and {} columns'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specs.csv file has 386 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "specs = pd.read_csv('../references/specs.csv')\n",
    "print('specs.csv file has {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lablels.csv file has 17690 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('../references/train_labels.csv/train_labels.csv')\n",
    "print('train_lablels.csv file has {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "#train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample submission file has 1000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv('../references/sample_submission.csv')\n",
    "print('sample submission file has {} rows and {} columns'.format(sample.shape[0], sample.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "Each installation_id is roughly equivalent to a user. let's see how many installation_ids there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = train.installation_id.unique()\n",
    "len(users) # number of distinct users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17000 unique installation_id's in the training set. Let's examine the breakdown of event types for a sample: `installation_id =='0001e90f'` (from the first row in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activity</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clip</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Game</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  counts\n",
       "0  Activity     469\n",
       "1      Clip       5\n",
       "2      Game     883"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = train.loc[train['installation_id']=='0001e90f'].groupby(['type'],as_index=False).size().reset_index(name='counts')\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below i calculate the total time spent on each different types of events in minutes for each installation_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c39453c56e1412bbcfeebce83ccb6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N=500    # how many users to aggregate over in train set\n",
    "user_times = np.zeros(N)\n",
    "times_per_type = [0,0,0,0]\n",
    "for j in tqdm(range(N)):\n",
    "    user = users[j]\n",
    "    event_types = [0,0,0,0]\n",
    "    sessions_typed = [0,0,0,0]\n",
    "    user_dat = train.loc[(train['installation_id']==user)]\n",
    "    event_types[0] = user_dat.loc[user_dat['type'] == 'Activity']\n",
    "    sessions_typed[0] = event_types[0]['game_session'].unique()\n",
    "    event_types[1] = user_dat.loc[user_dat['type'] == 'Clip']\n",
    "    sessions_typed[1] = event_types[1]['game_session'].unique()\n",
    "    event_types[2] = user_dat.loc[user_dat['type'] == 'Game']\n",
    "    sessions_typed[2] = event_types[2]['game_session'].unique()\n",
    "    event_types[3]   = user_dat.loc[user_dat['type'] == 'Assessment']\n",
    "    sessions_typed[3] = event_types[3]['game_session'].unique()\n",
    "        \n",
    "    for i in range(4):\n",
    "        type_time = 0.0\n",
    "        for session in sessions_typed[i]:\n",
    "            \n",
    "            #there may be an issue where there are NO events and then it doesn't figure this out??\n",
    "            sesh_start_ts = event_types[i].loc[event_types[i]['game_session']==session]['timestamp'].min()\n",
    "            sesh_start_dt = datetime.datetime.strptime(sesh_start_ts,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            sesh_end_ts = event_types[i].loc[event_types[i]['game_session']==session]['timestamp'].max()\n",
    "            sesh_end_dt = datetime.datetime.strptime(sesh_end_ts,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "            type_time = type_time + (sesh_end_dt - sesh_start_dt).total_seconds()/60.0\n",
    "            times_per_type[i]+=type_time\n",
    "        \n",
    "    total_time = sum(times_per_type)\n",
    "    user_times[j] = total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I plot the times in terms of the playing category -- we can see that kids spent most of the time playing games or activities and a little bit of time on assessments. Clip seems unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEICAYAAACTVrmbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de9wVZb338c9XQPF8RFMOYoSZlpKRmrbTsgzNQivbujXRLLKtmdUu6YhlPY/ZwdLMXpQotk2y1KTUkI2au/KESiIeHghPKCKGASYe0N/zx3WtGG7Xve4BZq3luvm+X695rZlrrmvmN3Ove35rZq41SxGBmZlZldZrdwBmZtb7OLmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycWaQlJ/SSFpUMn6G0h6RtIOzY7NzJrPyWUdkg/eteFlScsL00f30HaUpLkVxnKLpGNq0xHxfERsEhGPV7WOVxNJkyV9td1xGEjaRdKKdsfR2/VtdwDWOhGxSW1c0kPAxyPif9oXkTWDpL4R4YOntVdEeFgHB+Ah4N1dyjYEzgMWAPOB7wL9gK2B5cDLwDN52BrYD7gVWAI8DpwN9M3L6g8EMKjOur8PvAQ8l5f1/a71gcnAj4BpwD+BG4FtgZ8A/wBmA28qLHMwcBXwFDAPOLHBto8G7geWAY8Cp+TyUcBc4BvA4rycI7rsnx/mNk8A5wIbdGn7ZWAR8BhwdJ53CvAi8Hze3l/Xiam2/Sfnv80i4NuACnU+CTyQY7saGNil7aeAvwH311l+X+ByYGHefzcAry/Mn5y354a8X6bXWX63sXVZ12Tgq4XpUcDcwvTXSO+xpcB9wL/l8m7fT3n++4A5Of4fArcAx/S0f+rE92Tentp7eb8cy/BCnUHAs8AWa/O+WJeHtgfgoU1/+PrJ5Szgf4FtgO2A24Gv5HmrHCBy2V7AW4E+wLD8D3hintdtcsnzux4Y6iWXJ4A98j/vn/I/9b/n9X0XuDbX7QPMAk4D1gd2Bh4B9u9m3X8H9srjWwNvLmzjCuD/5uW8Ox9gdsrzfwr8Jh9wNgemAuMLbV8EvkJKyIeTDtKbFLbnq/Xi6bL9U/Pyd8rbe0yefyTpQLxzXv63gBu6tL06t92wzvL7AmOATXL984FbCvMnkw7ab8vzfwr8T5nY6qyr2+SS/57z8vtLwGsL+7fR++k1pERwaN7+L+b93eP+qRPfLsCKLmUTgW8Upk8jfwhYm/fFujy0PQAPbfrD108ujwHvKkyPJn8Kpk5yqbPMccClebyK5HJuYf4XgLsK028Fnsjj+wNzuiz/G8D53ax7IXA8sGmX8lGks6n+hbIped19gRcofBoG3gncV2i7BFivMH8pMKKwPWWSywGFss8BV+fxG8hnQnm6Xz64bldou+9q/P1fQzoT7V+I76LC/K3yMgf0FFudZTdKLruRzlreSeGspMT7aSyFZEG6X/wkK5NLt/unznLrJZf9WfXsahbwgbV9X6zLg2/oGwCSRDrgPFwofhgY2KDNrpKulbRQ0lLg66SznqosLIwvrzNdu4e0IzBU0j9qA+ng95pulnsY8CHgEUnXSxpZmLcoIp4rTD8M7JCHfsDswjp+S7pUV2z7cmH62UKMZT1aZ92QtvGnhXUvIn2aHtRN21VI6ivpe5Lm5b/V/aQzh63rtY+IxaQzhR3qze8SW2kRMZuUNL4NPCnpEknb5RgbvZ926BLfy6QPQzVl9k8jNwF9JL1N0ghge+Dawvy1eV+sk5xcDIBIH7meIP2T1gxh5T9w1Gn2M+BOYFhEbAZ8k3TAKrXKNQy1nkdJZ1hbFIZNI+LwuiuOuDkiDiV96r8OuLQwextJ/QvTQ0jX/xeQDlbDCuvYPCKKB+dGym7v4DrrhrSNx3XZxg0j4o6S6zgeOIj0qXpz0qd3WPXv9a91S9qKlBgXlIitq38CGxWmV0nyETEpIvYlXRLrT7qEBY3fTwsoJApJ67HqB58y++dfIbyiIL3/LwaOAT4KTI6IFwtVmvW+6LWcXKzoUmC8pK0lbUu6f/Dfed5CYFtJxU/imwJLIuIZSbsBn1iNdS0kHVyq8CcASafm79f0lbS7pD27VpS0saQjJW1GumyyjNS5oKYf8DVJ60t6F/Ae4PJ8oJkI/EjSNkoGS3pPyRjLbu9pkjaXNJR0A/1XufynwFclvT5vx5aSPlRy3ZD+Vs+R7jdtzMoDetFoSXtL2oCV9yyeLBFbVzOBQyVtIWkg8OnajHx2sn9ex/I81PZ/o/fTFGBvSYdI6ks6M92yMH919s+TpLOUIV3KLwY+AhyVx4ua9b7otZxcrOjrwL2knlgzgT+TbvID/JX0D/5wPv3fCvgs8HFJz5B6mXV3sKnnbOBYSU9LOqvH2g3kf/BDgH1JlysWkW5Yd3dJ6mO53hLgWNKN7pqHSJ9EnyAdNI6PiHl53qmkT6szcts/AK8rGeYE4K15301uUO9q0r6eAfyanNwj4lLgx8AV+ZLRTNIBrqwLSPvlCdL9hD/VqfPfwJmkHndvYNX90m1sdUwk3Yx/BPg9q54ZbkjqHfgU6VP/JqT3HTR4P0XEAtJB/5zcdlDejufz/NL7JyKeJr2v78h/jxG5/G+k3mbLIuK2Ls0eojnvi15L+QaU2TpP0ijgxxHR8gNDvuSyHBgcEfPbsP7JwD0R8YozmnbHVk8+e3kCeH9E3Fzhcn8J3FvcD+18X3Qyn7mYWUeQdHC+LNcfGE/qMFHvnsqaLv91pK7OF1a1zHWZk4uZdYp3AA+S7pkcCBweES9UseB8afYu4JsR8VhP9a1nvixmZmaV85mLmZlVzg+uzLbZZpsYOnRou8MwM+sod9xxx1MRMaBruZNLNnToUGbMmNHuMMzMOoqkh+uV+7KYmZlVzsnFzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXO39A363BDx13d7hDa6qEz39fuEKwOn7mYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrXtOQiabCkGyTdJ2m2pM/k8tMlPSZpZh4OKbT5kqS5kh6Q9N5C+ahcNlfSuEL5TpJulTRH0q8krZ/LN8jTc/P8oc3aTjMze6VmnrmsAD4fEW8A9gFOkrRrnnd2RIzIwzUAed6RwG7AKOAnkvpI6gOcBxwM7AocVVjOd/KyhgNPAyfk8hOApyPidcDZuZ6ZmbVI05JLRCyIiDvz+DLgPmBggyajgckR8XxEPAjMBfbKw9yImBcRLwCTgdGSBLwL+E1uPwk4rLCsSXn8N8CBub6ZmbVAS+655MtSbwZuzUUnS7pb0kRJW+aygcCjhWbzc1l35VsD/4iIFV3KV1lWnr8k1zczsxZoenKRtAlwOXBqRCwFzgeGASOABcD3a1XrNI81KG+0rK6xjZU0Q9KMRYsWNdwOMzMrr6nJRVI/UmK5JCKuAIiIhRHxUkS8DPyMdNkL0pnH4ELzQcDjDcqfAraQ1LdL+SrLyvM3BxZ3jS8iJkTEyIgYOWDAgLXdXDMzy5rZW0zABcB9EfGDQvn2hWqHA/fk8SnAkbmn107AcOA24HZgeO4Ztj7ppv+UiAjgBuDDuf0Y4KrCssbk8Q8D1+f6ZmbWAs18KvJ+wEeBWZJm5rIvk3p7jSBdpnoI+CRARMyWdBlwL6mn2UkR8RKApJOBqUAfYGJEzM7LOw2YLOlbwF2kZEZ+/YWkuaQzliObuJ1mZtZF05JLRPyJ+vc+rmnQ5tvAt+uUX1OvXUTMY+VltWL5c8ARqxOvmZlVx9/QNzOzyjm5mJlZ5ZxczMysck4uZmZWOScXMzOrnJOLmZlVzsnFzMwq5+RiZmaVc3IxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlXNyMTOzyjm5mJlZ5ZxczMysck4uZmZWOScXMzOrXN+eKkj6XJ3iJcAdETGz+pDMzKzTlTlzGQmcCAzMw1jgAOBnkr7YvNDMzKxTlUkuWwN7RsTnI+LzpGQzAHgHcFx3jSQNlnSDpPskzZb0mVy+laRpkubk1y1zuSSdI2mupLsl7VlY1phcf46kMYXyt0ialducI0mN1mFmZq1RJrkMAV4oTL8I7BgRy4HnG7RbAXw+It4A7AOcJGlXYBwwPSKGA9PzNMDBwPA8jAXOh5QogPHA3sBewPhCsjg/1621G5XLu1uHmZm1QJnk8kvgFknjJY0H/gxcKmlj4N7uGkXEgoi4M48vA+4jXVYbDUzK1SYBh+Xx0cDFkdwCbCFpe+C9wLSIWBwRTwPTgFF53mYRcXNEBHBxl2XVW4eZmbVAjzf0I+IMSdcC+wECToyIGXn20WVWImko8GbgVmC7iFiQl71A0ra52kDg0UKz+ay8z9Nd+fw65TRYR9e4xpLOfBgyZEiZTTEzsxJ6TC7ZXcDjtfqShkTEI2UaStoEuBw4NSKW5tsidavWKYs1KC8tIiYAEwBGjhy5Wm3NzKx7PV4Wk/RpYCHpctTvgavza48k9SMllksi4opcvDBf0iK/PpnL5wODC80HkRJao/JBdcobrcPMzFqgzD2XzwCvj4jdImL3iHhTROzeU6Pcc+sC4L6I+EFh1hSg1uNrDHBVofzY3GtsH2BJvrQ1FThI0pb5Rv5BwNQ8b5mkffK6ju2yrHrrMDOzFihzWexR0pcmV9d+wEeBWZJqX7b8MnAmcJmkE4BHgCPyvGuAQ4C5wLPA8QARsVjSGcDtud43I2JxHv8UcBGwIXBtHmiwDjMza4EyyWUecKOkqyl0Pe5yNvIKEfEn6t8XATiwTv0ATupmWROBiXXKZwBvrFP+93rrMDOz1iiTXB7Jw/p5MDMza6hMV+RvtCIQMzPrPbpNLpJ+GBGnSvoddbr4RsQHmhqZmZl1rEZnLr/Ir99rRSBmZtZ7dJtcIuKOPLopcE1EvNyakMzMrNOV+Z7LkcAcSWdJekOzAzIzs87XY3KJiGNIzwX7G3ChpJsljZW0adOjMzOzjlTqZ44jYinpMS6Tge2Bw4E786NhzMzMVlHm2WLvl3QlcD3QD9grIg4G9gD+q8nxmZlZByrzJcojgLMj4qZiYUQ8K+ljzQnLzMw6WZkvUR7bYN70asMxM7PeoMxlsX0k3S7pGUkvSHpJ0tJWBGdmZp2pzA39HwNHAXNITx/+OHBuM4MyM7POVuqXKCNirqQ+EfESqTvyX5ocl5mZdbAyyeVZSesDMyWdBSwANm5uWGZm1snKXBb7KNAHOBn4J+knhz/UzKDMzKyzlekt9nAeXQ748ft1DB13dbtDaKuHznxfu0Mws1eZRo/cn0WdR+3XRMTuTYnIzMw6XqMzl0NbFoWZmfUqjR65X7schqTXAHuRzmRuj4gnWhCbmZl1qDJfovw4cBvwQeDDwC1+7IuZmTVSpivyF4A3R8TfASRtDfwFmNjMwMzMrHOV6Yo8H1hWmF4GPNqccMzMrDcoc+byGHCrpKtI91xGA7dJ+hxARPygifGZmVkHKpNc/paHmqvyq3+J0szM6irzJUp/cdLMzFZLqZ85NjMzWx1OLmZmVrky33PZr0yZmZlZTZkzl3o/DOYfCzMzs251m1wkvU3S54EBkj5XGE4nPYK/IUkTJT0p6Z5C2emSHpM0Mw+HFOZ9SdJcSQ9Iem+hfFQumytpXKF8J0m3Spoj6Vf5N2eQtEGenpvnD13NfWJmZmup0ZnL+sAmpB5lmxaGpaTHwPTkImBUnfKzI2JEHq4BkLQrcCSwW27zE0l9JPUBzgMOBnYFjsp1Ab6TlzUceBo4IZefADwdEa8Dzs71zMyshRo9uPKPwB8lXVR8iGVZEXHTapw1jAYmR8TzwIOS5pIelAkwNyLmAUiaDIyWdB/wLuA/cp1JwOnA+XlZp+fy3wA/lqSI6PbnA8zMrFpl7rlsIGmCpOskXV8b1mKdJ0u6O1822zKXDWTVR8rMz2XdlW8N/CMiVnQpX2VZef6SXP8VJI2VNEPSjEWLFq3FJpmZWVGZ5PJr4C7gq6SHWNaGNXE+MAwYASwAvp/LVadurEF5o2W9sjBiQkSMjIiRAwYMaBS3mZmthjKPf1kREedXsbKIWFgbl/Qz4Pd5cj4wuFB1EPB4Hq9X/hSwhaS++eykWL+2rPmS+gKbA4uriN/MzMopc+byO0n/KWl7SVvVhjVZmaTtC5OHA7WeZFOAI3NPr52A4aTfkLkdGJ57hq1Puuk/Jd8/uYGVHQvGsPKZZ1PyNHn+9b7fYmbWWmXOXGoH6uKlsABe26iRpEuBA4BtJM0HxgMHSBqR2z8EfBIgImZLugy4F1gBnBQRL+XlnAxMJXV/nhgRs/MqTgMmS/oW6bLdBbn8AuAXuVPAYlJCMjOzFirz4Mqd1mTBEXFUneIL6pTV6n8b+Had8muAa+qUz2Nlj7Ji+XPAEasVrJmZVarM4182kvRVSRPy9HBJhzY/NDMz61Rl7rlcCLwA7Jun5wPfalpEZmbW8cokl2ERcRbwIkBELKd+d18zMzOgXHJ5QdKG5O+KSBoGPN/UqMzMrKOV6S02HvgDMFjSJcB+wHHNDMrMzDpbmd5i0yTdCexDuhz2mYh4qumRmZlZxypz5gKwP/B20qWxfsCVTYvIzMw6XpmuyD8BTgRmkb5R/0lJ5zU7MDMz61xlzlz2B95Ye4SKpEmkRGNmZlZXmd5iDwBDCtODgbubE46ZmfUGZc5ctgbuk3Rbnn4rcLOkKQAR8YFmBWdmZp2pTHL5etOjMDOzXqVMcpkBLI+IlyXtDOwCXBsRLzY3NDMz61Rl7rncBPSXNBCYDhwPXNTMoMzMrLOVSS6KiGeBDwLnRsThwG7NDcvMzDpZqeQi6W3A0cDVuaxP80IyM7NOVya5fAb4EnBl/sXI15J+YtjMzKyuMs8Wu4l036U2PQ84pZlBmZlZZytz5mJmZrZanFzMzKxyTi5mZla5Mk9FPkvSZpL6SZou6SlJx7QiODMz60xlzlwOioilwKHAfGBn4AtNjcrMzDpameTSL78eAlwaEYubGI+ZmfUCZZ4t9jtJ9wPLgf+UNAB4rrlhmZlZJ+vxzCUixgFvA0bmh1U+C4xudmBmZta5ytzQ3wg4CTg/F+0AjGxmUGZm1tnK3HO5EHgB2DdPzwe+1bSIzMys45VJLsMi4izgRYCIWA6oqVGZmVlHK5NcXpC0IRAAkoYBzzc1KjMz62hlksvpwB+AwZIuIf1g2Gk9NZI0UdKTku4plG0laZqkOfl1y1wuSedImivpbkl7FtqMyfXnSBpTKH+LpFm5zTmS1GgdZmbWOmV6i11H+qGw44BLSb3Gyjxy/yJgVJeyccD0iBhOSlLjcvnBwPA8jCV3HpC0FTAe2BvYCxhfSBbn57q1dqN6WIeZmbVImd5i0yPi7xFxdUT8PiKekjS9p3b5Uf1dv3A5GpiUxycBhxXKL47kFmALSdsD7wWmRcTiiHgamAaMyvM2i4ibIyKAi7ssq946zMysRbr9EqWk/sBGwDb5bKF2E38zUnfkNbFdRCwAiIgFkrbN5QOBRwv15ueyRuXz65Q3WscrSBpLOvthyJAha7hJZmbWVaNv6H8SOJWUSO4slC8Fzqs4jnq9z2INyldLREwAJgCMHDlytdubmVl93V4Wi4gfRcROwH9FxE6FYY+I+PEarm9hvqRFfn0yl88HBhfqDQIe76F8UJ3yRuswM7MWKdNbbImkY7sOa7i+KUCtx9cY4KpC+bG519g+wJJ8aWsqcJCkLfOluYOAqXneMkn75F5ix3ZZVr11mJlZi5R5cOVbC+P9gQNJl8kubtRI0qXAAaR7NvNJvb7OBC6TdALwCHBErn4N6anLc0nPLjseICIWSzoDuD3X+2bhqcyfIvVI2xC4Ng80WIeZmbVIj8klIj5dnJa0OfCLEu2O6mbWgXXqBun5ZfWWMxGYWKd8BvDGOuV/r7cOMzNrnTX5meNnSd8rMTMzq6vHMxdJv2NlT6z1gF2By5oZlJmZdbYy91y+VxhfATwcEfO7q2xmZlbmnssfWxGImZn1HmUe//LB/BDIJZKWSlomaWkrgjMzs85U5rLYWcD7I+K+ZgdjZma9Q5neYgudWMzMbHWUOXOZIelXwG8p/EhYRFzRtKjMzKyjlUkum5G+23JQoSwAJxczM6urTG+x41sRiJmZ9R6Nfs/lixFxlqRzqfM4+4g4pamRmZlZx2p05lK7iT+jFYGYmVnv0W1yiYjf5ddJ3dUxMzOrp8yzxUYCXwF2LNaPiN2bGJeZmXWwMr3FLgG+AMwCXm5uOGZm1huUSS6LImJK0yMxM7Neo0xyGS/p58B0/CVKMzMroUxyOR7YBejHysti/hKlmZl1q0xy2SMi3tT0SMzMrNco8+DKWyTt2vRIzMys1yhz5vJ2YIykB0n3XASEuyKbmVl3yiSXUU2PwszMepUyD658uBWBmJlZ71HmnouZmdlqcXIxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6uck4uZmVWuLclF0kOSZkmaKWlGLttK0jRJc/Lrlrlcks6RNFfS3ZL2LCxnTK4/R9KYQvlb8vLn5rZq/Vaama272nnm8s6IGBERI/P0OGB6RAwnPd5/XC4/GBieh7HA+ZCSETAe2BvYi/TTAFvmNufnurV2fsqAmVkLvZoui40GJuXxScBhhfKLI7kF2ELS9sB7gWkRsTgingamAaPyvM0i4uaICODiwrLMzKwF2pVcArhO0h2Sxuay7SJiAUB+3TaXDwQeLbSdn8salc+vU25mZi1S5sGVzbBfRDwuaVtgmqT7G9Std78k1qD8lQtOiW0swJAhQxpHbGZmpbXlzCUiHs+vTwJXku6ZLMyXtMivT+bq84HBheaDgMd7KB9Up7xeHBMiYmREjBwwYMDabpaZmWUtTy6SNpa0aW0cOAi4B5gC1Hp8jQGuyuNTgGNzr7F9gCX5stlU4CBJW+Yb+QcBU/O8ZZL2yb3Eji0sy8zMWqAdl8W2A67MvYP7Ar+MiD9Iuh24TNIJwCPAEbn+NcAhwFzgWeB4gIhYLOkM4PZc75sRsTiPfwq4CNgQuDYPZmbWIi1PLhExD9ijTvnfgQPrlAdwUjfLmghMrFM+A3jjWgdrZmZr5NXUFdnMzHoJJxczM6uck4uZmVXOycXMzCrn5GJmZpVzcjEzs8o5uZiZWeWcXMzMrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlXNyMTOzyjm5mJlZ5ZxczMysck4uZmZWuXb8zLGZ2avG0HFXtzuEtnvozPdVvkyfuZiZWeWcXMzMrHJOLmZmVjknFzMzq5yTi5mZVc7JxczMKufkYmZmlXNyMTOzyjm5mJlZ5ZxczMysck4uZmZWOScXMzOrnJOLmZlVrtcmF0mjJD0gaa6kce2Ox8xsXdIrk4ukPsB5wMHArsBRknZtb1RmZuuOXplcgL2AuRExLyJeACYDo9sck5nZOqO3/ljYQODRwvR8YO+ulSSNBcbmyWckPdCC2JphG+Cpdq1c32nXmivT1v3XC/j9t3ba/v5by324Y73C3ppcVKcsXlEQMQGY0PxwmkvSjIgY2e44OpX339rx/ls7vXX/9dbLYvOBwYXpQcDjbYrFzGyd01uTy+3AcEk7SVofOBKY0uaYzMzWGb3yslhErJB0MjAV6ANMjIjZbQ6rmTr+0l6bef+tHe+/tdMr958iXnErwszMbK301stiZmbWRk4uZmZWOSeXNpN0uKSQtEsP9Y6TtENh+uc9PXVA0l/y61BJ/1FNxJ1H0mskTZb0N0n3SrpG0s6S7snzR0o6p91xvhpI2k7SLyXNk3SHpJslHd7uuFqt7P9lp5H05Vaty8ml/Y4C/kTq0dbIccC/kktEfDwi7m3UICL2zaNDgXUyuUgScCVwY0QMi4hdgS8D29XqRMSMiDilXTG+WuR99Vvgpoh4bUS8hfS+HNTeyNqi7P9lp2lZciEiPLRpADYBHgN2Bu4vlH8RmAX8FTgT+DDwDPAAMBPYELgRGAl8Cjir0PY44Nw8/kx+vQVYktt+FvhfYEShzZ+B3du9P5q0j99FOlh2LR8K3JPHDwB+n8dPB34BXA/MAT7R7m1o4b46EPhjN/OG5vfNnXnYt7Dv/ghcBvy//H49Grgtv4eH5XoDgMtJXxO4Hdiv3dvbYD+84v8S2B64Kf8P3QP8G6kn6kV5ehbw2Vx3GPAH4I68z3bJ5Ufkun+tvSeB3fK+mgncDQzP+/p+4Oe5/iXAu/P/6Rxgr9x2Y2Bi3p93AaNz+XHAFTmGObXjQ/7bvJTXdUnT92O7/5Dr8gAcA1yQx/8C7El62OZfgI1y+Vb59UZgZKHtjaTkMoD0HLVa+bXA2/N4Lbn86+CZp8cAP8zjOwMz2r0vmriPTwHOrlM+lO6Ty19JCXwb0mOEdmj3drRzX+V5GwH98/jw2nsm77t/5IPvBqSD8jfyvM8U3me/LLwvhwD3tXt7G+yHev+Xnwe+ksv6AJsCbwGmFdptkV+nA8Pz+N7A9Xl8FjCwS91zgaPz+Pr5fTcUWAG8iXR16Q5SEhHpGYm/zfX/D3BMbXmk5L4xKbnMAzYH+gMPA4NzvWdatR975fdcOshRwA/z+OQ8vR5wYUQ8CxARixstICIW5evj+5A+pbye9AmnkV8DX5P0BeBjpE9fttJVEbEcWC7pBtKDUH/b5phaTtJ5wNuBF0ifnH8saQTp0+/Ohaq3R8SC3OZvwHW5fBbwzjz+bmDXdOUNgM0kbRoRy5q7FWuk3v/l74CJkvqRDu4zJc0DXivpXOBq4DpJmwD7Ar8ubOsG+fXPwEWSLiOdWQDcDHxF0iDgioiYk9s9GBGzACTNBqZHREiaRUo+AAcBH5D0X3m6Pylxk+svye3vJT3/q/i8xaZzcmkTSVuTLtm8UVKQPg0F6dLB6n756FfAR0in0ldG/ojSnYh4VtI00qegj5DOgHqr2aTLiquj6/5bV74MNhv4UG0iIk6StA0wg3Q5dSGwB+kD0HOFds8Xxl8uTL/MymPMesDbcmC6Us0AAAJCSURBVNJ+1Wrwf/lF4B3A+4BfSPpuRFwsaQ/gvcBJpP+lU4F/RMSIrsuOiBMl7Z2XMVPSiIj4paRbc9lUSR8nnXWU2acCPhQRqzxwN6+j2P4l2nCs9w399vkwcHFE7BgRQyNiMPAgsBj4mKSNACRtlesvI52K13MFcBjpE9av6syv1/bnwDmkT50Nz4463PXABpI+USuQ9Fa6eZJrNlpS/3ygOYB0TXtdcD3QX9KnCmUb5dfNgQUR8TLwUdJBd3VcB5xcm8hnQK9G3f1fvgN4MiJ+BlwA7JkT73oRcTnwNWDPiFgKPCjpCEidJHICQtKwiLg1Ir5OegryYEmvBeZFxDmkR1TtvhqxTgU+nTtiIOnNJdq8mM++ms7JpX2OIvViKrqc1CNsCjBD0kygdsp7EfBTSTMlbVhsFBFPA/cCO0bEbXXWdTewQtJfJX02t7kDWApcWNH2vCrls7jDgffkrsizSfdVGj3I9DbSZY5bgDMiYp146GneV4cB+0t6UNJtwCTgNOAnwBhJt5Auif1zNRd/CjBS0t35Ms2JFYZepe7+Ly8inW3cRTq7+xHppz1uzP+nFwFfyvWPBk6Q9FfS2WDtt6S+K2lW7gJ/E+ne3r8D9+Rl7AJcvBqxngH0A+7OyzyjRJsJuf4lq7GeNeLHv6yj8ndmbiT1ZHm5zeG8akg6nXTT83vtjsWsk/nMZR0k6VjgVlLvFycWM6ucz1zMzKxyPnMxM7PKObmYmVnlnFzMzKxyTi5mZlY5JxczM6vc/wdHqjVvgohWdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p2 = plt.bar([1,2,3,4], times_per_type);\n",
    "\n",
    "plt.title('Total time spent per app usage type');\n",
    "plt.ylabel('minutes spent playing')\n",
    "plt.xticks([1,2,3,4], ('Activity', 'Clip', 'Game', 'Assessment'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ground truth\n",
    "\n",
    "Note that the Bird Measurer assessment has two parts however we consider the it to be passed correctly if just the first part is passed, that is the 4110 code. Here, I duplicate the train_labels that were provided by parsing the train data set and calculating the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments = train.loc[(train['type'] == 'Assessment') & (((train['event_code'] == 4100)&(train['title'] != 'Bird Measurer (Assessment)')) | (train['event_code'] == 4110))].copy()\n",
    "assessments['num_correct'] = True\n",
    "\n",
    "test_assessments = test.loc[(test['type'] == 'Assessment') & (((test['event_code'] == 4100)&(test['title'] != 'Bird Measurer (Assessment)')) | (test['event_code'] == 4110))].copy()\n",
    "test_assessments_for_union = test_assessments.copy()\n",
    "\n",
    "test_assessments['num_correct'] = True    # initialize all assessments as succeeding\n",
    "\n",
    "assessments.loc[assessments['event_data'].str.contains(\":false,\"),'num_correct'] = False   # update labels where user incorrect\n",
    "\n",
    "test_assessments.loc[test_assessments['event_data'].str.contains(\":false,\"),'num_correct'] = False  # ditto on test set\n",
    "\n",
    "assessments['num_incorrect'] = np.where(assessments.num_correct > 0,0,1)   # label incorrect\n",
    "\n",
    "test_assessments['num_incorrect'] = np.where(test_assessments.num_correct > 0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I calculate the number of incorrect assessments is using a groupby(). We need to know both number incorrect as well as if there was a correct answer in separate columns. I chose two separately create these two columns and then merge them using an inner join, however this is probably not best practice, there is probably a way to create two aggregate columns in a single step without creating two dataframes (stackoverflow this) perhaps using the agg() function  for multiple aggregations - that might do the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_correct'].sum().sort_values(by=['installation_id'])\n",
    "h = assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_incorrect'].sum().sort_values(by=['installation_id'])\n",
    "\n",
    "ggg = test_assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_correct'].sum().sort_values(by=['installation_id'])\n",
    "hhh = test_assessments.groupby(['game_session','installation_id','title'],as_index=False)['num_incorrect'].sum().sort_values(by=['installation_id'])\n",
    "\n",
    "g['num_correct'] = g['num_correct'].astype(int)      #g.shape\n",
    "h['num_incorrect'] = h['num_incorrect'].astype(int)   #h.shape\n",
    "\n",
    "ggg['num_correct'] = ggg['num_correct'].astype(int)      #g.shape\n",
    "hhh['num_incorrect'] = hhh['num_incorrect'].astype(int)   #h.shape\n",
    "\n",
    "merged_inner = pd.merge(left=g,right=h, left_on='game_session', right_on='game_session')\n",
    "\n",
    "test_merged_inner = pd.merge(left=ggg,right=hhh, left_on='game_session', right_on='game_session')\n",
    "\n",
    "df1 = merged_inner[['game_session','installation_id_x','title_x','num_correct','num_incorrect']]\n",
    "df2 = test_merged_inner[['game_session','installation_id_x','title_x','num_correct','num_incorrect']] \n",
    "pd.options.mode.chained_assignment = None\n",
    "df1.rename(columns = {'installation_id_x':'installation_id'}, inplace = True)\n",
    "df1.rename(columns = {'title_x':'title'}, inplace = True)\n",
    "df2.rename(columns = {'installation_id_x':'installation_id'}, inplace = True)\n",
    "df2.rename(columns = {'title_x':'title'}, inplace = True)\n",
    "\n",
    "df1['accuracy'] = df1.apply(lambda row: row.num_correct/(row.num_correct + row.num_incorrect), axis=1)\n",
    "df2['accuracy'] = df2.apply(lambda row: row.num_correct/(row.num_correct + row.num_incorrect), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not the same\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define accuracy groups\n",
    "\n",
    "def groupacc(row):\n",
    "    if row['num_correct'] == 1 and row['num_incorrect'] == 0:\n",
    "        return 3\n",
    "    if row['num_correct'] == 1 and row['num_incorrect'] == 1:\n",
    "        return 2\n",
    "    if row['num_correct'] == 1 and row['num_incorrect'] > 1:\n",
    "        return 1\n",
    "    if row['num_correct'] == 0: \n",
    "        return 0\n",
    "    return 'Other'\n",
    "\n",
    "df1['accuracy_group'] = df1.apply(groupacc, axis=1)\n",
    "df2['accuracy_group'] = df2.apply(groupacc, axis=1)\n",
    "\n",
    "# I did a check comparing my results to the provided train_labels\n",
    "\n",
    "c = df1.sort_values(by=['game_session'])\n",
    "k = train_labels.sort_values(by=['game_session'])\n",
    "\n",
    "comparison_array = c.values == k.values\n",
    "\n",
    "if False in comparison_array:\n",
    "    print (\"Not the same\")\n",
    "    \n",
    "len(np.where(comparison_array==False)[0])  # 46 differences due to floating point stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "The target, e.g. `y`, is the accuracy group for an assessment session, e.g. this is what you are trying to predict.\n",
    "The features, `X` are collected by looking up the user's historical data for each assessment session, you only want to consider data up to the current timestamp of the assessment you are training or predicting on (except for the type of assessment). Though it would be possible to incorporate historical data from test set into train, this would leak data from test into the model and also might hurt the build process mixing things up too much since the private test set is still withheld. It's a thought though, but probably wrong.\n",
    "\n",
    "\n",
    "Let's pair down the train set to only the ids that actually took (e.g. started 2000) an assessment and also made an attempt (4100/4110) and do some more inspection, with comments detailing below. One issue is features which have NA for some data - (accumulated previous accuracy group) -> our approach will be to substitue the mean value for the type. Using 0 is another way which may produce better results. Below I start by doing some encodings.  We need to predict the score for the last assessment in the test set. Also, despite some unclear language in the instructions, after some effort it became clear to me from inspection that **the last assessment (by timestamp) has been truncated for each user in the test set**. Some of the test set users have no previous assessments to learn from. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17690\n",
      "3614\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3237436270939549,\n",
       " 2.3004095398699107,\n",
       " 2.306781750924784,\n",
       " 0.8383092921838309,\n",
       " 2.2142667021559754]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the assessment title\n",
    "\n",
    "list_of_assessment_titles = list(set(assessments['title'].unique()))\n",
    "list_of_assessment_titles.sort()\n",
    "\n",
    "assessment_titles_map = dict(zip(list_of_assessment_titles, np.arange(len(list_of_assessment_titles))))\n",
    "assessment_titles_map\n",
    "\n",
    "assessments['title'] = assessments['title'].map(assessment_titles_map)    # note if you run this twice it will break\n",
    "train_labels['title'] = train_labels['title'].map(assessment_titles_map)  # note if you run this twice it will break\n",
    "\n",
    "print(len(assessments['game_session'].unique()))  # unique sessions of type assesment in train set\n",
    "train_reduced_users = assessments['installation_id'].unique()\n",
    "print(len(train_reduced_users))  # unique users who made at least one attempt on an assessment in train\n",
    "\n",
    "train = train.loc[train['installation_id'].isin(train_reduced_users)]  # drop some data where user has no assessment\n",
    "#train.shape\n",
    "\n",
    "test_users = test.installation_id.unique()    # installation_ids in test set\n",
    "print(len(test_users))\n",
    "len(set(test_users).intersection(set(users)))  # there is no intersection in installation_ids between train and test\n",
    "\n",
    "# Combine the historical test portion with the predict set: which is the last assessment (no 4100/4110 code)\n",
    "\n",
    "test_predict = test.loc[test['type'] == 'Assessment'].groupby('installation_id',as_index=False).last()\n",
    "\n",
    "# you need to combine test_assessments so that it has those last rows basically do a union/concat at this stage\n",
    "\n",
    "test_final_assessments = pd.concat([test_assessments_for_union,test_predict],sort=False)\n",
    "test_final_assessments = test_final_assessments.sort_values(by=['installation_id','timestamp'])\n",
    "\n",
    "# calculate mean group accuracies on train\n",
    "\n",
    "mean_group_accs = train_labels.groupby('title',as_index=False).mean()['accuracy_group']\n",
    "list(mean_group_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to build train or test features. I don't think this code is efficient. In future consider using python\n",
    "# Counter() function as a good resource. Also do some unncessary extra work looping through entire time history for each user \n",
    "# instead of working incrementally would definitely be more efficient\n",
    "\n",
    "def build_features(num,test_or_train):\n",
    "\n",
    "    N=num   # number of users to build data for, to go to production set N = len(train/test_users)\n",
    "    features = []   # this will by the full data for each assessment   X  & Y\n",
    "    build_test = test_or_train   # False\n",
    "\n",
    "    for j in tqdm(range(N)):\n",
    "\n",
    "        if build_test == False:\n",
    "            user = train_reduced_users[j]   \n",
    "            prev_hist = train.loc[train['installation_id']==user]   # for previous history counts, narrowed to user    \n",
    "            user_assessments = assessments.loc[assessments['installation_id']==user]  # for previous cum_accuracy_grps    \n",
    "            user_sessions = user_assessments['game_session'].unique()  # time ordered\n",
    "        else:\n",
    "            user = test_users[j]\n",
    "            prev_hist = test.loc[test['installation_id']==user]   # for previous history counts, narrowed to user \n",
    "            user_sessions = test_final_assessments.loc[test_final_assessments['installation_id']==user]['game_session'].unique()\n",
    "\n",
    "        counters = np.array([0.0,0.0,0.0,0.0,0.0])  # number of each type of assessment initialization\n",
    "        cumalitive_acc_groups = np.array(list(mean_group_accs))   # initialization\n",
    "        #cumalitive_acc_groups = np.array([0.0,0.0,0.0,0.0,0.0])   # alternative zero initialization\n",
    "\n",
    "        magmapeak_counts = 0\n",
    "        treetopcity_counts = 0\n",
    "        crystalcaves_counts = 0\n",
    "\n",
    "        game_counts = 0\n",
    "        clip_counts = 0\n",
    "        activity_counts = 0\n",
    "        assess_counts = 0\n",
    "\n",
    "        ii = 1\n",
    "\n",
    "        for session in user_sessions:     # these are already time-ordered \n",
    "\n",
    "            # code to convert timestamp data to datetime data\n",
    "            prev_hist['timestamp'] = pd.to_datetime(prev_hist['timestamp'])  # get the time at the start of the game_session\n",
    "            the_time = prev_hist.loc[prev_hist['game_session']==session]['timestamp'].iloc[0]\n",
    "            the_past = prev_hist.loc[prev_hist['timestamp'] < the_time]  # this could be empty!! \n",
    "            num_events = the_past.groupby('world',as_index=False).size().reset_index(name='counts')\n",
    "            num_sessions_by_type = the_past.groupby('game_session',as_index=False).last().groupby('type').size().reset_index(name='tcounts')\n",
    "\n",
    "            for index, row in num_sessions_by_type.iterrows():\n",
    "                if row['type'] == 'Game':\n",
    "                    game_counts = row['tcounts']\n",
    "                if row['type'] == 'Activity':\n",
    "                    activity_counts = row['tcounts']\n",
    "                if row['type'] == 'Assessment':\n",
    "                    assess_counts = row['tcounts'] \n",
    "                if row['type'] == 'Clip':\n",
    "                    clip_counts = row['tcounts']\n",
    "\n",
    "            # now you want to count number of previous events for this user before this timestamp\n",
    "\n",
    "            user_features = []\n",
    "\n",
    "            user_features.append(clip_counts)\n",
    "            user_features.append(activity_counts)\n",
    "            user_features.append(assess_counts)\n",
    "            user_features.append(game_counts)  \n",
    "\n",
    "            if build_test == False:\n",
    "                sesh_dat = train_labels.loc[train_labels['game_session'] == session]\n",
    "                score = sesh_dat['accuracy_group'].iloc[0]\n",
    "            else:\n",
    "                if ii < len(user_sessions):\n",
    "                    sesh_dat = df2.loc[df2['game_session'] == session]\n",
    "                    score = sesh_dat['accuracy_group'].iloc[0]\n",
    "                else:\n",
    "                    score = 0         \n",
    "\n",
    "            title = prev_hist.loc[prev_hist['game_session']==session]['title'].iloc[0]   \n",
    "            title = assessment_titles_map[title]\n",
    "            \n",
    "            #user_features.append(user)\n",
    "            user_features.append(title)\n",
    "\n",
    "            total_accum = np.sum(cumalitive_acc_groups)/(np.sum(counters)+5.0)\n",
    "            user_features.append(cumalitive_acc_groups[0]/(counters[0]+1.0))\n",
    "            user_features.append(cumalitive_acc_groups[1]/(counters[1]+1.0))\n",
    "            user_features.append(cumalitive_acc_groups[2]/(counters[2]+1.0))\n",
    "            user_features.append(cumalitive_acc_groups[3]/(counters[3]+1.0))\n",
    "            user_features.append(cumalitive_acc_groups[4]/(counters[4]+1.0))\n",
    "\n",
    "            if title == 0:\n",
    "                counters[0] += 1\n",
    "                cumalitive_acc_groups[0] = cumalitive_acc_groups[0] + score\n",
    "            elif title == 1:\n",
    "                counters[1] += 1\n",
    "                cumalitive_acc_groups[1] = cumalitive_acc_groups[1] + score\n",
    "            elif title == 2:\n",
    "                counters[2] += 1\n",
    "                cumalitive_acc_groups[2] = cumalitive_acc_groups[2] + score\n",
    "            elif title == 3:\n",
    "                counters[3] += 1\n",
    "                cumalitive_acc_groups[3] = cumalitive_acc_groups[3] + score\n",
    "            elif title == 4:\n",
    "                counters[4] += 1\n",
    "                cumalitive_acc_groups[4] = cumalitive_acc_groups[4] + score\n",
    "\n",
    "            user_features.append(total_accum)\n",
    "\n",
    "            user_features.append(score)\n",
    "\n",
    "            for index, row in num_events.iterrows():\n",
    "                if row['world'] == 'MAGMAPEAK':\n",
    "                    magmapeak_counts = row['counts']\n",
    "                if row['world'] == 'TREETOPCITY':\n",
    "                    treetopcity_counts = row['counts']\n",
    "                if row['world'] == 'CRYSTALCAVES':\n",
    "                    crystalcaves_counts = row['counts']  \n",
    "\n",
    "            user_features.append(magmapeak_counts)\n",
    "            user_features.append(treetopcity_counts)\n",
    "            user_features.append(crystalcaves_counts)\n",
    "            user_features.append(sum(num_events['counts']) - (magmapeak_counts + treetopcity_counts+crystalcaves_counts))\n",
    "            user_features.append(sum(num_events['counts']))\n",
    "            \n",
    "            if build_test == False:\n",
    "                features.append(user_features)\n",
    "            else:\n",
    "                if ii == len(user_sessions):\n",
    "                    features.append(user_features)\n",
    "            ii += 1\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build the train and test set. Alternatively, if we already built the train we can simply load it from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c678f01129b042dc94eb2f534815ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame(build_features(len(test_users),True))\n",
    "test = test.round(2)\n",
    "\n",
    "#train = pd.DataFrame(build_features(len(train_reduced_users),False))\n",
    "#train = train.round(2)\n",
    "\n",
    "#train.to_csv('train.csv', index=False)\n",
    "\n",
    "# to read in features that are already built uncomment belwo\n",
    "#train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alot more features could be included such as some **durations**... alot more could be done here but for now we just want to go ahead and get it to work and then we can come back to feature engineering later if we have time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature transformations\n",
    "\n",
    "I did not do any feature or target transformations. One idea for example is to log transform numerical data. You should consider this in later work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "\n",
    "y_train = train['11']\n",
    "y_test = test[11]\n",
    "X_train = train.drop('11', axis=1)\n",
    "X_test = test.drop(11, axis=1)\n",
    "\n",
    "categorical = 4         # categoricals\n",
    "#categorical = '4'   #  if you load train it reads header as string (alternatively cast header to int)\n",
    "\n",
    "# or do this for some models\n",
    "\n",
    "X_train['4'] = X_train['4'].astype('category') \n",
    "\n",
    "X_test[4]= X_test[4].astype('category') \n",
    "\n",
    "# dummy variable encoding (not always desired)\n",
    "\n",
    "dumb = pd.get_dummies(X_train['4'],drop_first=True,prefix='g')\n",
    "dumby = pd.get_dummies(X_test[categorical],drop_first=True,prefix='j')\n",
    "\n",
    "X_train = X_train.drop(['4'],axis=1)\n",
    "X_test = X_test.drop([categorical],axis=1)\n",
    "\n",
    "X_train = pd.concat([X_train,dumb],axis=1)\n",
    "X_test = pd.concat([X_test,dumby],axis=1)\n",
    "\n",
    "\n",
    "# Feature scaling - if desired for model\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "X_test_scaled = preprocessing.scale(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation metric -  quadratic weighted kappa (qwk)\n",
    "\n",
    "This is a metric commonly used for measuring the aggreement between two raters where bigger differences are penalized (the squared). The outer product is simply coming from the assuming independent marginals on true and predicted sets (from histograms) and then computing the independent joint probability by multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwk(a1,a2):\n",
    "    N = 4\n",
    "    o = confusion_matrix(a1,a2)\n",
    "    w = np.zeros(shape=(N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            w[i,j] = (i-j)**2\n",
    "    w = w/((N-1)*(N-1)) \n",
    "    w = w.round(3)\n",
    "    \n",
    "    e = np.outer(np.histogram(a1, [i for i in range(N+1)])[0],np.histogram(a2,[i for i in range(N+1)])[0])\n",
    "\n",
    "    e = e/np.sum(e)\n",
    "    o = o/np.sum(o)\n",
    "\n",
    "    return 1 - np.sum(np.multiply(o,w))/np.sum(np.multiply(e,w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Models\n",
    "\n",
    "\n",
    "Our approach to modelling is to work from simple models to complex models and to learn about how to implement each model correctly while building cross-validation into the pipeline as well. The idea for this competition is to learn how to build models, not neccessarily to win this competition... though that would be nice... I was able to successfully run the following models\n",
    "\n",
    "1. KNN\n",
    "\n",
    "2. Support Vector Machine\n",
    "\n",
    "3. Single decision tree \n",
    "\n",
    "4. Gradient boosted tree ensemble - regression with thresholding\n",
    "\n",
    "\n",
    "Models that I did not get time to try but which would be good\n",
    "\n",
    "1. Neural Networks\n",
    "\n",
    "2. Ridge/Logistic/Lasso regressions with/without thresholding\n",
    "\n",
    "3. Multi-Layer Perceptrons\n",
    "\n",
    "Different models handle categorical and numerical data differently in terms of feature transformations and scaling/preprocessing, this should be kept in mind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbors\n",
    "\n",
    "Below we do cross-validation to see what value of k (the hyperparameter) optimizes. Smaller values of k lead to overfitting.\n",
    "KNN is the dumbest of models but provides a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85328fa0c314dcda551dc15999177d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_scores = []\n",
    "nns = []\n",
    "stds = []\n",
    "qwks_mean = []\n",
    "for nn in tqdm(range(2,30,5)):\n",
    "    \n",
    "    cv = KFold(n_splits=5)\n",
    "    neigh = KNeighborsClassifier(n_neighbors = nn)\n",
    "    \n",
    "    scores = []\n",
    "    qwks = []\n",
    "\n",
    "    # if you are dealing with dataframes, you have to index with X.iloc[test_index] etc. !!1\n",
    "    for train_index, test_index in cv.split(X_scaled,y_train):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train_cv, X_test_cv = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        neigh.fit(X_train_cv, y_train_cv)\n",
    "        scores.append(neigh.score(X_test_cv, y_test_cv))\n",
    "        y_pred = neigh.predict(X_test_cv)\n",
    "        qwk_res = qwk(y_pred,y_test_cv)\n",
    "        qwks.append(qwk_res)\n",
    "    qwks_mean.append(np.mean(qwks))\n",
    "    scores\n",
    "    nns.append(nn)\n",
    "    stds.append(np.std(scores))\n",
    "    mean_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nns,mean_scores);\n",
    "plt.xlabel(\"k\");plt.ylabel(\"cv-mean-accuracy\");\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.plot(nns,qwks_mean);\n",
    "plt.xlabel('k');\n",
    "plt.ylabel('qwk_mean');\n",
    "\n",
    "# final submit chose k = 20\n",
    "neigh = KNeighborsClassifier(n_neighbors = 20)\n",
    "neigh.fit(X_scaled, y_train)\n",
    "y_pred = neigh.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model yielded a score of **0.425** on the private test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "We repeat the cross-validation strategy over the hyperparameter C for support vector machine. We also require the data to be standardized for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f520da26d646568e62d4e82a5dec43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_scores = []\n",
    "nns = []\n",
    "stds = []\n",
    "qwks_mean = []\n",
    "Cs = [0.1,0.25,0.5,0.75,1.0]\n",
    "\n",
    "for c in tqdm(range(5)):\n",
    "    \n",
    "    cv = KFold(n_splits=5)\n",
    "    clf = SVC(C=Cs[c],gamma='auto')\n",
    "\n",
    "    scores = []\n",
    "    qwks = []\n",
    "    # if you are dealing with dataframes, you have to index with X.iloc[test_index] etc. !!1\n",
    "    for train_index, test_index in cv.split(X_scaled,y_train):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train_cv, X_test_cv = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        clf.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        scores.append(clf.score(X_test_cv, y_test_cv))\n",
    "        \n",
    "        y_pred = clf.predict(X_test_cv)\n",
    "        qwk_res = qwk(y_pred,y_test_cv)\n",
    "        qwks.append(qwk_res)\n",
    "        \n",
    "    qwks_mean.append(np.mean(qwks))\n",
    "    stds.append(np.std(scores))\n",
    "    mean_scores.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Cs,mean_scores);\n",
    "plt.xlabel(\"k\");plt.ylabel(\"cv-mean-accuracy\");\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.plot(Cs,qwks_mean);\n",
    "plt.xlabel('k');\n",
    "plt.ylabel('qwk_mean');\n",
    "\n",
    "# final submit chose C = 0.75\n",
    "clf = SVC(C = 0.75)\n",
    "clf.fit(X_scaled, y_train)\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model yielded a score of **0.43** on the private test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Classifier\n",
    "\n",
    "Here we try a simple single decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "stds = []\n",
    "qwks_mean = []\n",
    "\n",
    "depths = [2,4,6,8,10,12,14,16,18,20]\n",
    "\n",
    "for depth in depths:\n",
    "    \n",
    "    cv = KFold(n_splits=10)\n",
    "    clf = DecisionTreeClassifier(random_state=0,max_depth=depth)\n",
    "\n",
    "    scores = []\n",
    "    qwks = []\n",
    "\n",
    "    # if you are dealing with dataframes, you have to index with X.iloc[test_index] etc. !!1\n",
    "    for train_index, test_index in cv.split(X_train,y_train):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        clf.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        scores.append(clf.score(X_test_cv, y_test_cv))\n",
    "        y_pred = clf.predict(X_test_cv)                \n",
    "        qwk_res = qwk(y_pred,y_test_cv)                 \n",
    "        qwks.append(qwk_res)\n",
    "        \n",
    "    qwks_mean.append(np.mean(qwks))\n",
    "    stds.append(np.std(scores))\n",
    "    mean_scores.append(np.mean(scores))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths,mean_scores);\n",
    "plt.xlabel(\"depth\");plt.ylabel(\"cv-mean-accuracy\");\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.plot(depths,qwks_mean);\n",
    "plt.xlabel(\"depth\")\n",
    "plt.ylabel(\"qwk\")\n",
    "\n",
    "# final submit chose max_depth = 7\n",
    "clf = DecisionTreeClassifier(random_state=0,max_depth=7)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)   # do you want feature scaling?\n",
    "\n",
    "#submission = {'installation_id': test_users, 'accuracy_group': y_pred}\n",
    "#sample_submission = pd.DataFrame(d)\n",
    "#sample_submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discussion and Takeaways\n",
    "\n",
    "There's quite a few things I could have done better (or could have just done at all).\n",
    "\n",
    "1. I found that the lack of design of the build process for train and test sets led to problems later. In general both design and efficient coding are paramount. The problem was that there were some differences in the build pipeline for test set compared to train. This has to do with the rules of the kaggle competition and how it was structured. Avoid the thematic **error** of not designing the train and test feature building process - do your homework to make this integrated and scalable!!\n",
    "\n",
    "\n",
    "2. You can actually average the results from multiple models, and then threshold, quote below is from teh 8th place finisher\n",
    "\n",
    "\"Simple average 18 of predictions of 9 models (2 outputs, acc and acc_group per model). then use threshold Optimizer to find thresholds. I randomly initiallized the thresholds for threshold Optimizer around training target distribution, and ran threshold Optimizer 25 times, then chose the one with best cv qwk.\n",
    "I did a 5 fold simulation(4 folds act as oof we have, 1 fold acts as label of test data) to compare several ways of deciding thresholds. Found that using threshold Optimizer is better than deciding thresholds by simple using training target distributiion.\"\n",
    "\n",
    "3. You can use NN or perceptron models!!\n",
    "\n",
    "4. Feature transformations - log transforms of numeric data\n",
    "\n",
    "5. Feature interactions\n",
    "\n",
    "Another thing I didn't do very well was simply organizing my code. It's very important to separate each step and to block your code into separate compartments **of appropriate length, you simply had too small juypyter cells, too many distinct cell code blocks unnesseccarily for a scattered effect** having too long blocks, would also be bad ('spaghetti code')\n",
    "\n",
    "Another problem I noticed while reviewing my code, is that I named some variables like `gggg` or `hh` instead of giving descriptive names. If you find yourself having to do this, then maybe you are creating temporary variable unneccesarily and can improve the design with nested, more efficient codes. Otherwise, avoid the thematic **error** of using non-descriptive names!!\n",
    "\n",
    "another thing: avoid thematic **error** of not deleting/dropping unneeded objects to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
